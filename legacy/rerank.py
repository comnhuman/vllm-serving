# app.py
# Requires: transformers>=4.51.0, fastapi, uvicorn, torch
import os
os.environ["HF_HOME"] = "./huggingface_data"

from typing import List, Optional

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM

import uvicorn

# APP_TITLE = "Qwen3-Reranker-8B (yes/no) FastAPI"
# MODEL_ID = "Qwen/Qwen3-Reranker-8B"
APP_TITLE = "Qwen3-Reranker-4B (yes/no) FastAPI"
MODEL_ID = "Qwen/Qwen3-Reranker-4B"
# APP_TITLE = "Qwen3-Reranker-0.6B (yes/no) FastAPI"
# MODEL_ID = "Qwen/Qwen3-Reranker-0.6B"

# -------- Model load (once) --------
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side="left")
use_flash = os.getenv("QWEN_USE_FLASH_ATTENTION", "0") == "1"
device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "cpu"

model_kwargs = {}
if use_flash:
    # pipë¡œ flash-attn ì„¤ì¹˜ í•„ìš” (ì˜µì…˜)
    model_kwargs.update(dict(attn_implementation="flash_attention_2"))

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    **model_kwargs
).to(device).eval()

# yes/no í† í° ì•„ì´ë”” (ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ë‹¨ì¼ í† í° ê°€ì •)
token_false_id = tokenizer.convert_tokens_to_ids("no")
token_true_id  = tokenizer.convert_tokens_to_ids("yes")

# í”„ë¡¬í”„íŠ¸ ì¡°ê°
SYSTEM_PROMPT = (
    "<|im_start|>system\n"
    'Judge whether the Document meets the requirements based on the Query and the Instruct provided. '
    'Note that the answer can only be "yes" or "no".'
    "<|im_end|>\n<|im_start|>user\n"
)
ASSISTANT_SUFFIX = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

# ë¯¸ë¦¬ í† í¬ë‚˜ì´ì¦ˆ
prefix_tokens = tokenizer.encode(SYSTEM_PROMPT, add_special_tokens=False)
suffix_tokens = tokenizer.encode(ASSISTANT_SUFFIX, add_special_tokens=False)

DEFAULT_INSTRUCT = "Given a web search query, retrieve relevant passages that answer the query"
DEFAULT_MAX_LENGTH = 8192

# -------- FastAPI --------
app = FastAPI(title=APP_TITLE)


# ---------- Schemas ----------
class RerankRequest(BaseModel):
    queries: List[str]
    documents: List[str]
    instruction: Optional[str] = None
    max_length: Optional[int] = None  # ì—†ìœ¼ë©´ ê¸°ë³¸ 8192 ì‚¬ìš©


class RerankResponse(BaseModel):
    scores: List[float]
    used_pairs: List[str]  # ë””ë²„ê¹…/ì¬í˜„ìš©(ìš”ì²­ ì‹œí€€ìŠ¤ ê·¸ëŒ€ë¡œ)


# ---------- Core helpers (ì›ë³¸ ë¡œì§ì„ í•¨ìˆ˜ë¡œ í¬íŒ…) ----------
def format_instruction(instruction: Optional[str], query: str, doc: str) -> str:
    if instruction is None:
        instruction = DEFAULT_INSTRUCT
    return "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
        instruction=instruction, query=query, doc=doc
    )


def process_inputs(pairs: List[str], max_length: int):
    # prefix/suffixë¥¼ ê³ ë ¤í•œ í† í° ê¸¸ì´ ì œí•œ
    available = max_length - len(prefix_tokens) - len(suffix_tokens)
    if available <= 8:
        raise ValueError("max_lengthê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. prefix/suffixë¥¼ í¬í•¨í•´ ìµœì†Œ ìˆ˜ì‹­ í† í° ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")

    inputs = tokenizer(
        pairs,
        padding=False,
        truncation="longest_first",
        return_attention_mask=False,
        max_length=available,
    )

    # prefix/suffix ë¶€ì°©
    for i, ids in enumerate(inputs["input_ids"]):
        inputs["input_ids"][i] = prefix_tokens + ids + suffix_tokens

    # pad ë° í…ì„œ ë³€í™˜
    inputs = tokenizer.pad(inputs, padding=True, return_tensors="pt", max_length=max_length)

    for key in inputs:
        inputs[key] = inputs[key].to(device)
    return inputs


@torch.no_grad()
def compute_logits(inputs) -> List[float]:
    # ë§ˆì§€ë§‰ í† í° ë¡œì§“ì—ì„œ yes/no í™•ë¥  ë¹„êµ
    logits = model(**inputs, use_cache=False).logits[:, -1, :]
    true_vec = logits[:, token_true_id]
    false_vec = logits[:, token_false_id]
    batch_scores = torch.stack([false_vec, true_vec], dim=1)          # [N, 2]
    batch_scores = torch.nn.functional.log_softmax(batch_scores, 1)   # log-softmax
    scores = batch_scores[:, 1].exp().tolist()                        # "yes" í™•ë¥ 
    return scores


# ---------- Routes ----------
@app.get("/health")
def health():
    return {
        "status": "ok",
        "model_id": MODEL_ID,
        "device": device,
        "flash_attention": use_flash,
    }


@app.post("/rerank", response_model=RerankResponse)
def rerank(req: RerankRequest):
    if not req.queries or not req.documents:
        raise HTTPException(status_code=400, detail="queriesì™€ documentsëŠ” ê°ê° 1ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    if len(req.queries) != len(req.documents):
        raise HTTPException(status_code=400, detail="queriesì™€ documentsì˜ ê¸¸ì´ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.")

    pairs = [format_instruction(req.instruction or DEFAULT_INSTRUCT, q, d)
             for q, d in zip(req.queries, req.documents)]

    max_len = int(req.max_length or DEFAULT_MAX_LENGTH)
    try:
        inputs = process_inputs(pairs, max_len)
        with torch.inference_mode():
            scores = compute_logits(inputs)
        response = RerankResponse(scores=scores, used_pairs=pairs)
        return response

    finally:
        # ğŸ’¡ ì‘ë‹µ ìƒì„± ì´í›„ ì‹¤í–‰ë¨ â€” ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            del inputs, scores
        except Exception:
            pass
        if torch.cuda.is_available():
            torch.cuda.empty_cache()



@app.get("/")
def root():
    return {
        "message": "Use POST /rerank with queries & documents.",
        "docs": "/docs",
        "health": "/health",
    }

if __name__ == "__main__":
    uvicorn.run(
        app,        # ëª¨ë“ˆì´ë¦„:ê°ì²´ì´ë¦„
        host="0.0.0.0",   # ì™¸ë¶€ ì ‘ì† í—ˆìš©
        port=9806,        # í¬íŠ¸
        reload=False       # ì½”ë“œ ë³€ê²½ ì‹œ ìë™ ì¬ì‹œì‘ (ê°œë°œìš©)
    )